{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ce39f8-54e1-46e1-a529-9975bedd08fa",
   "metadata": {},
   "source": [
    "## This code reads the .dat files generated after running the PANDAT simulations using panpython. It can read large number of files as it runs in parallel. The data is stored in a pickle format as large number of rows and columns cannot be accomodated in excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebdafd-d8fd-49e2-bd89-3ccd9f1eac40",
   "metadata": {},
   "source": [
    "### Importing all the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45c2f7-ba89-488b-8b19-abfaf6116e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#Define the path where files are placed\n",
    "files_path = glob.glob(\"output/dat_files/*.dat\")\n",
    "print(f'Total number of dat files are {len(files_path)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45359c9-3ce9-4bf1-835b-b8b8ffbca95b",
   "metadata": {},
   "source": [
    "### Defining a function to read all the dat files defined above and append it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2da5e-adb7-444b-b495-e712c4f784cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_concatenated_dataframe(file_paths):\n",
    "    \"\"\"\n",
    "    Reads multiple CSV files and concatenates their contents into a single Pandas DataFrame.    \n",
    "    Returns:\n",
    "    A Pandas DataFrame containing the combined contents of all CSV files.\n",
    "    \"\"\"\n",
    "    # Create a list of DataFrames from the CSV files\n",
    "    dfs = [pd.read_csv(file_path, sep='\\t') for file_path in file_paths] # Reads all the dat files described lying on files_path\n",
    "    dfs = [df.tail(-1) for df in dfs] #Deleting the first row of each dataframe as it contains the units row, not necessary to read\n",
    "    # Concatenate the DataFrames into a single DataFrame\n",
    "    concatenated_df = pd.concat(dfs) # Concatenating the dataframes\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c653cb-19c5-45b7-b0c2-b2073ba977f5",
   "metadata": {},
   "source": [
    "### Running the above function in parallel over all the files contained in file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcedd0-d8a4-41cc-bbb7-4ebfeff1a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the time counter\n",
    "ini = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(16) as executor:\n",
    "    futures = [executor.submit(read_csv_to_concatenated_dataframe, [file_path]) for file_path in files_path]\n",
    "    dfs = [future.result() for future in futures] # Getting the concatenated dataframes after running the above function on multiple threads for each thread\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dfs) # Concatenating all the dataframes obtained after running the function on multiple threads\n",
    "\n",
    "# Dropping the completely empty rows. Empty rows are generated when two dataframes are concatenated. \n",
    "final_df = final_df.dropna(subset=['phase_name'], how = 'all')\n",
    "fin = time.time()\n",
    "time_taken = round(fin-ini, 2)\n",
    "\n",
    "print(f\"Time taken to execute function and get the final dataframe is: {time_taken} seconds\")\n",
    "print(f'Number of rows and columns for the final data are {final_df.shape[0]} and {final_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119cabd-4452-4ec3-be3b-435ced5055cf",
   "metadata": {},
   "source": [
    "### Getting the total memory of the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a264e7-1d0d-447c-a1a2-ebcadccffb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = []\n",
    "a = final_df.memory_usage(deep = True)\n",
    "for i in range(len(a)):\n",
    "    _ = a[i]\n",
    "    memory.append(_)\n",
    "ar = np.array(memory)\n",
    "ar = round((ar.sum()/1073741824),2)\n",
    "print(f'Size of dataframe is {ar} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6e692-6aea-4b62-8a1b-77b712cf9a5a",
   "metadata": {},
   "source": [
    "### Importing the final dataframe into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01204ea0-b2b2-40fd-9e8d-5cffdbb2ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = time.time()\n",
    "\n",
    "# Write dataframe to pickle file\n",
    "with open('final_data_test.pickle', 'wb') as f:\n",
    "    pickle.dump(final_df, f)\n",
    "\n",
    "fin = time.time()\n",
    "time_taken = round(fin-ini, 2)\n",
    "print(f\"Time taken to write a picke file is: {time_taken} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5d3ad0c-4e36-4fc3-9e7d-e8c3864ee561",
   "metadata": {},
   "source": [
    "#Template for reading the pickle file\n",
    "\n",
    "ini = time.time()\n",
    "# Read dataframe from pickle file\n",
    "with open('final.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "fin = time.time()\n",
    "print(fin-ini)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63040cc-2f6a-4b16-af5a-66b3b5aa7654",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
